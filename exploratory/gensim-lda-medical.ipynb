{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA and pyLDAvis with medical transcriptions and AWS SageMaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Gratefully indebted to my sources, from whom I have lifted quite a bit of the code in this notebook:\n",
    "* https://github.com/priya-dwivedi/Deep-Learning/blob/master/topic_modeling/LDA_Newsgroup.ipynb\n",
    "* https://github.com/XuanX111/Friends_text_generator/blob/master/Friends_LDAvis_Xuan_Qi.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (3.8.3)\n",
      "Requirement already satisfied: six>=1.5.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from gensim) (1.14.0)\n",
      "Requirement already satisfied: numpy>=1.11.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from gensim) (1.18.1)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from gensim) (1.4.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from gensim) (3.0.0)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from smart-open>=1.8.1->gensim) (2.22.0)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests->smart-open>=1.8.1->gensim) (1.25.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests->smart-open>=1.8.1->gensim) (2020.6.20)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests->smart-open>=1.8.1->gensim) (2.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests->smart-open>=1.8.1->gensim) (3.0.4)\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.2.4 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/python3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyLDAvis in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (2.1.2)\n",
      "Requirement already satisfied: future in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from pyLDAvis) (0.18.2)\n",
      "Requirement already satisfied: joblib>=0.8.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from pyLDAvis) (0.14.1)\n",
      "Requirement already satisfied: numpy>=1.9.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from pyLDAvis) (1.18.1)\n",
      "Requirement already satisfied: pandas>=0.17.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from pyLDAvis) (1.0.1)\n",
      "Requirement already satisfied: jinja2>=2.7.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from pyLDAvis) (2.11.1)\n",
      "Requirement already satisfied: scipy>=0.18.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from pyLDAvis) (1.4.1)\n",
      "Requirement already satisfied: wheel>=0.23.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from pyLDAvis) (0.34.2)\n",
      "Requirement already satisfied: pytest in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from pyLDAvis) (5.3.5)\n",
      "Requirement already satisfied: funcy in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from pyLDAvis) (1.15)\n",
      "Requirement already satisfied: numexpr in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from pyLDAvis) (2.7.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from pandas>=0.17.0->pyLDAvis) (2019.3)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from pandas>=0.17.0->pyLDAvis) (2.8.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from jinja2>=2.7.2->pyLDAvis) (1.1.1)\n",
      "Requirement already satisfied: py>=1.5.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from pytest->pyLDAvis) (1.8.1)\n",
      "Requirement already satisfied: packaging in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from pytest->pyLDAvis) (20.1)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from pytest->pyLDAvis) (19.3.0)\n",
      "Requirement already satisfied: more-itertools>=4.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from pytest->pyLDAvis) (8.2.0)\n",
      "Requirement already satisfied: pluggy<1.0,>=0.12 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from pytest->pyLDAvis) (0.13.1)\n",
      "Requirement already satisfied: wcwidth in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from pytest->pyLDAvis) (0.1.8)\n",
      "Requirement already satisfied: importlib-metadata>=0.12 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from pytest->pyLDAvis) (1.5.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from python-dateutil>=2.6.1->pandas>=0.17.0->pyLDAvis) (1.14.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from packaging->pytest->pyLDAvis) (2.4.6)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from importlib-metadata>=0.12->pytest->pyLDAvis) (2.2.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.2.4 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/python3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from gensim import models\n",
    "from gensim.corpora import Dictionary, MmCorpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Loading nltk libraries\n",
    "'''\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "\n",
    "from nltk.corpus import stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(400)\n",
    "np.set_printoptions(precision=3, suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accessing the SageMaker Python SDK\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker.amazon.common import numpy_to_record_serializer\n",
    "from sagemaker.predictor import csv_serializer, json_deserializer\n",
    "from sagemaker import get_execution_role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from IPython.core.debugger import set_trace\n",
    "\n",
    "import tempfile\n",
    "import string\n",
    "import funcy as fp\n",
    "import sys\n",
    "import logging\n",
    "import os\n",
    "import pickle\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim as gensimvis\n",
    "import pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(filename='gensim.log',\n",
    "                    format=\"%(asctime)s:%(levelname)s:%(message)s\",\n",
    "                    level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['description', 'medical_specialty', 'sample_name', 'transcription',\n",
      "       'keywords'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "      <th>medical_specialty</th>\n",
       "      <th>sample_name</th>\n",
       "      <th>transcription</th>\n",
       "      <th>keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A 23-year-old white female presents with comp...</td>\n",
       "      <td>Allergy / Immunology</td>\n",
       "      <td>Allergic Rhinitis</td>\n",
       "      <td>SUBJECTIVE:,  This 23-year-old white female pr...</td>\n",
       "      <td>allergy / immunology, allergic rhinitis, aller...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Consult for laparoscopic gastric bypass.</td>\n",
       "      <td>Bariatrics</td>\n",
       "      <td>Laparoscopic Gastric Bypass Consult - 2</td>\n",
       "      <td>PAST MEDICAL HISTORY:, He has difficulty climb...</td>\n",
       "      <td>bariatrics, laparoscopic gastric bypass, weigh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Consult for laparoscopic gastric bypass.</td>\n",
       "      <td>Bariatrics</td>\n",
       "      <td>Laparoscopic Gastric Bypass Consult - 1</td>\n",
       "      <td>HISTORY OF PRESENT ILLNESS: , I have seen ABC ...</td>\n",
       "      <td>bariatrics, laparoscopic gastric bypass, heart...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2-D M-Mode. Doppler.</td>\n",
       "      <td>Cardiovascular / Pulmonary</td>\n",
       "      <td>2-D Echocardiogram - 1</td>\n",
       "      <td>2-D M-MODE: , ,1.  Left atrial enlargement wit...</td>\n",
       "      <td>cardiovascular / pulmonary, 2-d m-mode, dopple...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2-D Echocardiogram</td>\n",
       "      <td>Cardiovascular / Pulmonary</td>\n",
       "      <td>2-D Echocardiogram - 2</td>\n",
       "      <td>1.  The left ventricular cavity size and wall ...</td>\n",
       "      <td>cardiovascular / pulmonary, 2-d, doppler, echo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         description  \\\n",
       "0   A 23-year-old white female presents with comp...   \n",
       "1           Consult for laparoscopic gastric bypass.   \n",
       "2           Consult for laparoscopic gastric bypass.   \n",
       "3                             2-D M-Mode. Doppler.     \n",
       "4                                 2-D Echocardiogram   \n",
       "\n",
       "             medical_specialty                                sample_name  \\\n",
       "0         Allergy / Immunology                         Allergic Rhinitis    \n",
       "1                   Bariatrics   Laparoscopic Gastric Bypass Consult - 2    \n",
       "2                   Bariatrics   Laparoscopic Gastric Bypass Consult - 1    \n",
       "3   Cardiovascular / Pulmonary                    2-D Echocardiogram - 1    \n",
       "4   Cardiovascular / Pulmonary                    2-D Echocardiogram - 2    \n",
       "\n",
       "                                       transcription  \\\n",
       "0  SUBJECTIVE:,  This 23-year-old white female pr...   \n",
       "1  PAST MEDICAL HISTORY:, He has difficulty climb...   \n",
       "2  HISTORY OF PRESENT ILLNESS: , I have seen ABC ...   \n",
       "3  2-D M-MODE: , ,1.  Left atrial enlargement wit...   \n",
       "4  1.  The left ventricular cavity size and wall ...   \n",
       "\n",
       "                                            keywords  \n",
       "0  allergy / immunology, allergic rhinitis, aller...  \n",
       "1  bariatrics, laparoscopic gastric bypass, weigh...  \n",
       "2  bariatrics, laparoscopic gastric bypass, heart...  \n",
       "3  cardiovascular / pulmonary, 2-d m-mode, dopple...  \n",
       "4  cardiovascular / pulmonary, 2-d, doppler, echo...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the data\n",
    "df = pd.read_csv('mtsamples.csv').drop(['Unnamed: 0'], axis=1)\n",
    "\n",
    "print(df.columns)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['transcription'].isnull().sum()\n",
    "df=df.dropna(subset=['transcription']).copy()\n",
    "df['transcription'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Surgery', 'Consult - History and Phy.', 'Cardiovascular / Pulmonary']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['medical_specialty']=df['medical_specialty'].str.strip()\n",
    "spec_list = df['medical_specialty'].value_counts().head(3).index.tolist()\n",
    "spec_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "surgery = df[df['medical_specialty']==spec_list[0]]['transcription']\n",
    "consult = df[df['medical_specialty']==spec_list[1]]['transcription']\n",
    "cardio = df[df['medical_specialty']==spec_list[2]]['transcription']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A) Surgery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1. Data Preprocessing\n",
    "We will perform the following steps:\n",
    "\n",
    "* Tokenization: Split the text into sentences and the sentences into words. Lowercase the words and remove punctuation.\n",
    "* Words that have fewer than 3 characters are removed.\n",
    "* All stopwords are removed.\n",
    "* Words are lemmatized - words in third person are changed to first person and verbs in past and future tenses are changed into present.\n",
    "* Words are stemmed - words are reduced to their root form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "specialty=surgery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Write a function to perform the pre processing steps on the entire dataset\n",
    "'''\n",
    "\n",
    "# Tokenize \n",
    "def preprocess(text):\n",
    "    result=[]\n",
    "    for token in gensim.utils.simple_preprocess(text) :\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(token.lower())           \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1088\n"
     ]
    }
   ],
   "source": [
    "processed_docs = []\n",
    "for doc in specialty:\n",
    "    processed_docs.append(preprocess(doc))\n",
    "print(len(processed_docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2. Corpus and dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the corpus\n",
    "\n",
    "def nltk_stopwords():\n",
    "    return set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "def prep_corpus(docs, additional_stopwords=set(), no_below=3, no_above=0.5):\n",
    "    print('Building dictionary...')\n",
    "    dictionary = Dictionary(docs)\n",
    "#     print(dictionary)\n",
    "    stopwords = nltk_stopwords().union(additional_stopwords)\n",
    "#     print(stopwords)\n",
    "    stopword_ids = map(dictionary.token2id.get, stopwords)\n",
    "#     print(stopword_ids)\n",
    "    dictionary.filter_tokens(stopword_ids)\n",
    "#     print(dictionary)\n",
    "    dictionary.compactify()\n",
    "#     print(dictionary)\n",
    "    dictionary.filter_extremes(no_below=no_below, no_above=no_above, keep_n=None)\n",
    "\n",
    "    \n",
    "    print('Building corpus...')\n",
    "    corpus = [dictionary.doc2bow(doc) for doc in docs]\n",
    "    \n",
    "    return dictionary, corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building dictionary...\n",
      "Building corpus...\n"
     ]
    }
   ],
   "source": [
    "dictionary, corpus = prep_corpus(processed_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3. Modeling & Visualization with pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = models.ldamodel.LdaModel(corpus=corpus,\n",
    "         id2word=dictionary,\n",
    "         num_topics=10,\n",
    "         eval_every=10,\n",
    "         passes=50,\n",
    "         iterations=5000,\n",
    "         random_state=np.random.RandomState(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_data1 = gensimvis.prepare(lda_model, corpus, dictionary)\n",
    "surgery_lda = open('surgery.html', 'w')\n",
    "pyLDAvis.save_html(vis_data1, surgery_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B) Consultations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "specialty=consult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Write a function to perform the pre processing steps on the entire dataset\n",
    "'''\n",
    "\n",
    "# Tokenize \n",
    "def preprocess(text):\n",
    "    result=[]\n",
    "    for token in gensim.utils.simple_preprocess(text) :\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(token.lower())           \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "516\n"
     ]
    }
   ],
   "source": [
    "processed_docs = []\n",
    "for doc in specialty:\n",
    "    processed_docs.append(preprocess(doc))\n",
    "print(len(processed_docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2. Corpus and dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the corpus\n",
    "\n",
    "def nltk_stopwords():\n",
    "    return set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "def prep_corpus(docs, additional_stopwords=set(), no_below=3, no_above=0.5):\n",
    "    print('Building dictionary...')\n",
    "    dictionary = Dictionary(docs)\n",
    "#     print(dictionary)\n",
    "    stopwords = nltk_stopwords().union(additional_stopwords)\n",
    "#     print(stopwords)\n",
    "    stopword_ids = map(dictionary.token2id.get, stopwords)\n",
    "#     print(stopword_ids)\n",
    "    dictionary.filter_tokens(stopword_ids)\n",
    "#     print(dictionary)\n",
    "    dictionary.compactify()\n",
    "#     print(dictionary)\n",
    "    dictionary.filter_extremes(no_below=no_below, no_above=no_above, keep_n=None)\n",
    "\n",
    "    \n",
    "    print('Building corpus...')\n",
    "    corpus = [dictionary.doc2bow(doc) for doc in docs]\n",
    "    \n",
    "    return dictionary, corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building dictionary...\n",
      "Building corpus...\n"
     ]
    }
   ],
   "source": [
    "dictionary, corpus = prep_corpus(processed_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3. Modeling & Visualization with pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = models.ldamodel.LdaModel(corpus=corpus,\n",
    "         id2word=dictionary,\n",
    "         num_topics=10,\n",
    "         eval_every=10,\n",
    "         passes=50,\n",
    "         iterations=5000,\n",
    "         random_state=np.random.RandomState(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_data1 = gensimvis.prepare(lda_model, corpus, dictionary)\n",
    "consult_lda = open('consult.html', 'w')\n",
    "pyLDAvis.save_html(vis_data1, consult_lda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C) Cardio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1. Data Preprocessing\n",
    "We will perform the following steps:\n",
    "\n",
    "* Tokenization: Split the text into sentences and the sentences into words. Lowercase the words and remove punctuation.\n",
    "* Words that have fewer than 3 characters are removed.\n",
    "* All stopwords are removed.\n",
    "* Words are lemmatized - words in third person are changed to first person and verbs in past and future tenses are changed into present.\n",
    "* Words are stemmed - words are reduced to their root form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "specialty=cardio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Write a function to perform the pre processing steps on the entire dataset\n",
    "'''\n",
    "\n",
    "# Tokenize \n",
    "def preprocess(text):\n",
    "    result=[]\n",
    "    for token in gensim.utils.simple_preprocess(text) :\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(token.lower())           \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "371\n"
     ]
    }
   ],
   "source": [
    "processed_docs = []\n",
    "for doc in specialty:\n",
    "    processed_docs.append(preprocess(doc))\n",
    "print(len(processed_docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2. Corpus and dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the corpus\n",
    "\n",
    "def nltk_stopwords():\n",
    "    return set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "def prep_corpus(docs, additional_stopwords=set(), no_below=3, no_above=0.5):\n",
    "    print('Building dictionary...')\n",
    "    dictionary = Dictionary(docs)\n",
    "#     print(dictionary)\n",
    "    stopwords = nltk_stopwords().union(additional_stopwords)\n",
    "#     print(stopwords)\n",
    "    stopword_ids = map(dictionary.token2id.get, stopwords)\n",
    "#     print(stopword_ids)\n",
    "    dictionary.filter_tokens(stopword_ids)\n",
    "#     print(dictionary)\n",
    "    dictionary.compactify()\n",
    "#     print(dictionary)\n",
    "    dictionary.filter_extremes(no_below=no_below, no_above=no_above, keep_n=None)\n",
    "\n",
    "    \n",
    "    print('Building corpus...')\n",
    "    corpus = [dictionary.doc2bow(doc) for doc in docs]\n",
    "    \n",
    "    return dictionary, corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building dictionary...\n",
      "Building corpus...\n"
     ]
    }
   ],
   "source": [
    "dictionary, corpus = prep_corpus(processed_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3. Modeling & Visualization with pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = models.ldamodel.LdaModel(corpus=corpus,\n",
    "         id2word=dictionary,\n",
    "         num_topics=10,\n",
    "         eval_every=10,\n",
    "         passes=50,\n",
    "         iterations=5000,\n",
    "         random_state=np.random.RandomState(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_data1 = gensimvis.prepare(lda_model, corpus, dictionary)\n",
    "cardio_lda = open('cardio.html', 'w')\n",
    "pyLDAvis.save_html(vis_data1, cardio_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
